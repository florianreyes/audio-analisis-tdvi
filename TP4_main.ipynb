{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad Torcuato Di Tella\n",
    "\n",
    "Licenciatura en Tecnología Digital\\\n",
    "**Tecnología Digital VI: Inteligencia Artificial**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "import torchaudio\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio.datasets import GTZAN\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seteamos data_dir con el path a los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blues',\n",
       " 'classical',\n",
       " 'country',\n",
       " 'disco',\n",
       " 'hiphop',\n",
       " 'jazz',\n",
       " 'metal',\n",
       " 'pop',\n",
       " 'reggae',\n",
       " 'rock']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_dir = 'genres_5sec'\n",
    "list_files = os.listdir(data_dir)\n",
    "classes = []\n",
    "for file in list_files:\n",
    "  name = '{}/{}'.format(data_dir, file)\n",
    "  if os.path.isdir(name):\n",
    "    classes.append(file)\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 22050\n",
    "\n",
    "\n",
    "def parse_genres(fname):\n",
    "    parts = fname.split('/')[-1].split('.')[0]\n",
    "    return parts  # ' '.join(parts[0])\n",
    "\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files = []\n",
    "        for c in classes:\n",
    "          self.files = self.files + \\\n",
    "              [fname for fname in os.listdir(\n",
    "                  os.path.join(root, c)) if fname.endswith('.wav')]\n",
    "        self.classes = list(set(parse_genres(fname) for fname in self.files))\n",
    "        #self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        fname = self.files[i]\n",
    "\n",
    "        #img = self.transform(open_image(fpath))\n",
    "        genre = parse_genres(fname)\n",
    "        fpath = os.path.join(self.root, genre, fname)\n",
    "        class_idx = self.classes.index(genre)\n",
    "        audio = torchaudio.load(fpath)[0]\n",
    "\n",
    "        return audio, class_idx\n",
    "\n",
    "\n",
    "dataset = MusicDataset(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790, 100, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "val_size = 100\n",
    "test_size = 100\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    dataset, [train_size, val_size, test_size])\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 20\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True,\n",
    "                      num_workers=4, pin_memory=True)\n",
    "valid_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, 1, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters: 25290\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "cnn = M5(n_input=1, n_output=len(classes))\n",
    "cnn.to(device)\n",
    "print(cnn)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(cnn)\n",
    "print(\"Number of parameters: %s\" % n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 12760, 2060, 15852, 16296) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1133\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\queue.py:178\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_empty\u001b[39m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Intel\\Desktop\\audio-analisis-tdvi\\TP4_main.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Intel/Desktop/audio-analisis-tdvi/TP4_main.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Intel/Desktop/audio-analisis-tdvi/TP4_main.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m cnn\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Intel/Desktop/audio-analisis-tdvi/TP4_main.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m wav, genre_index \u001b[39min\u001b[39;00m train_dl:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Intel/Desktop/audio-analisis-tdvi/TP4_main.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Intel/Desktop/audio-analisis-tdvi/TP4_main.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     wav \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1283\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1284\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1285\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1286\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{\u001b[39;00mpids_str\u001b[39m}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1147\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 12760, 2060, 15852, 16296) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.0005)\n",
    "valid_losses = []\n",
    "num_epochs = 30\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()  # importante para ir liberando memoria ram\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    # Train\n",
    "    cnn.train()\n",
    "    for wav, genre_index in train_dl:\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        wav = wav.to(device)\n",
    "        genre_index = torch.as_tensor(genre_index).to(device)\n",
    "\n",
    "        # Forward\n",
    "        out = cnn(wav)\n",
    "        #M5\n",
    "        loss = F.nll_loss(out.squeeze(), genre_index)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        del wav  # importante para ir liberando memoria ram\n",
    "        del genre_index  # importante para ir liberando memoria ram\n",
    "        del loss  # importante para ir liberando memoria ram\n",
    "        del out  # importante para ir liberando memoria ram\n",
    "        torch.cuda.empty_cache()  # importante para ir liberando memoria ram\n",
    "        gc.collect()  # importante para ir liberando memoria ram\n",
    "\n",
    "    print('Epoch: [%d/%d], Train loss: %.4f' %\n",
    "          (epoch+1, num_epochs, np.mean(losses)))\n",
    "\n",
    "    # Validation\n",
    "    cnn.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    for wav, genre_index in valid_dl:\n",
    "        #print(wav, genre, index)\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        out = cnn(wav)\n",
    "        #M5\n",
    "        loss = F.nll_loss(out.squeeze(), genre_index)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        #M5\n",
    "        pred = out.argmax(dim=-1).flatten()\n",
    "        # append labels and predictions\n",
    "        correct += pred.eq(genre_index).sum().item()\n",
    "        y_true.extend(genre_index)\n",
    "        y_pred.extend(pred)\n",
    "        del wav  # importante para ir liberando memoria ram\n",
    "        del genre_index  # importante para ir liberando memoria ram\n",
    "        del loss  # importante para ir liberando memoria ram\n",
    "        del out  # importante para ir liberando memoria ram\n",
    "        torch.cuda.empty_cache()  # importante para ir liberando memoria ram\n",
    "        gc.collect()  # importante para ir liberando memoria ram\n",
    "\n",
    "    accuracy = correct / len(valid_dl.dataset)\n",
    "    valid_loss = np.mean(losses)\n",
    "    print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy: %.4f' %\n",
    "          (epoch+1, num_epochs, valid_loss, accuracy))\n",
    "\n",
    "    # Save model\n",
    "    valid_losses.append(valid_loss.item())\n",
    "    if np.argmin(valid_losses) == epoch:\n",
    "        print('Saving the best model at %d epochs!' % epoch)\n",
    "        torch.save(cnn.state_dict(), 'best_model.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, 1, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "S = torch.load('best_model.ckpt')\n",
    "cnn.load_state_dict(S)\n",
    "print('loaded!')\n",
    "\n",
    "# Run evaluation\n",
    "cnn.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for wav, genre_index in test_dl:\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        out = cnn(wav)\n",
    "\n",
    "        pred = out.argmax(dim=-1).flatten()\n",
    "        # append labels and predictions\n",
    "        correct += pred.eq(genre_index).sum().item()\n",
    "        y_true.extend(genre_index)\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "accuracy = correct / len(test_dl.dataset)\n",
    "print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy: %.4f' %\n",
    "      (epoch+1, num_epochs, valid_loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, label = test_dl.dataset[12]\n",
    "print(\"shape of waveform {}, sample rate with {}, label is {} \".format(\n",
    "    waveform.size(), samplerate, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(waveform, rate=22050)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = torch.unsqueeze(waveform, dim=0)\n",
    "cnn.to(device)\n",
    "wav = wav.to(device)\n",
    "out = cnn(wav)\n",
    "pred = out.argmax(dim=-1).flatten()\n",
    "classes[pred], classes[label]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
